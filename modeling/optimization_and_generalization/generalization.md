[2003.01054] [Double Trouble in Double Descent : Bias and Variance(s) in the Lazy Regime](https://arxiv.org/abs/2003.01054)

[2002.08709] [Do We Need Zero Training Loss
After Achieving Zero Training Error?](https://arxiv.org/abs/2002.08709)

[2002.04710] [Unique Properties of Wide Minima in Deep Networks](https://arxiv.org/abs/2002.04710)

[1912.02292] [Deep Double Descent: Where Bigger Models and More Data Hurt](https://arxiv.org/abs/1912.02292)

[1712.09913] [Visualizing the Loss Landscape of Neural Nets](https://arxiv.org/abs/1712.09913)

[1611.03530] [Understanding deep learning requires rethinking generalization](https://arxiv.org/abs/1611.03530)
