[NIPS2019] [Control Batch Size and Learning Rate to Generalize Well: Theoretical and Empirical Evidence]()

[1711.07240] [MegDet: A Large Mini-Batch Object Detector](https://arxiv.org/abs/1711.07240)

[1711.00489] [Don't Decay the Learning Rate, Increase the Batch Size](https://arxiv.org/abs/1711.00489)

[1709.05011] [ImageNet Training in Minutes](https://arxiv.org/abs/1709.05011)

[1706.02677] [Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour](https://arxiv.org/abs/1706.02677)

[1705.08741] [Train longer, generalize better: closing the
generalization gap in large batch training of neural
networks](https://arxiv.org/abs/1705.08741)

[1609.04836] [On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima](https://arxiv.org/abs/1609.04836)
  - Flat loss landscape leads to better generalization

[1608.03983] [SGDR: Stochastic Gradient Descent with Warm Restarts](https://arxiv.org/abs/1608.03983)

[1606.04838] [Optimization Methods for Large-Scale Machine Learning](https://arxiv.org/abs/1606.04838)
