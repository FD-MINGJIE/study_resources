### 2020
- [AAAI2020] [Decoupled Attention Network for Text Recognition](https://arxiv.org/abs/1912.10205)
  - **short summary**
    - narrow advantage
  - annotation
    - word level
  - rectification module
    - todo
  - feature map
    - 2d
  - decoder
    - RNN
  - motivation
    - there is misalignment between the ground truth strings and the attention’s output sequences of probability distribution, which is caused by missing or superfluous characters
  - solution & novelty
    - decouple the alignment operation from using historical decoding results
    - alignment is achived by a spatial attention module
  - the devil in the details
    - None
### 2019
- [CVPR2019] [Aggregation Cross-Entropy for Sequence Recognition](https://arxiv.org/abs/1904.08364)
  - **short summary**
    - unfair comparison
    - interesting idea
  - annotation
    - characters and their count in the sequence annotation  
  - rectification module
    - todo
  - feature map
    - 2d
  - decoder
    - ACE
  - motivation
    - CTC and RNN based mechanism is not easy enough
  - solution & novelty
    - proposes the ACE loss function
      - much quicker implementation, faster inference/back-propagation, approximately O(1) in parallel
      - less storage requirement (no parameter and negligible runtime memory)
      - convenient employment (by replacing CTC with ACE)
      - competitive performance to CTC and the attention mechanism
  - the devil in the details
    - None  
### 2018
- [CVPR2018] [Edit Probability for Scene Text Recognition](https://arxiv.org/abs/1805.03384)
  - **short summary**
    - todo
  - annotation
    - word level
  - rectification module
    - todo
  - feature map
    - 1d
  - decoder
    - RNN and attention
  - motivation
    - there is misalignment between the ground truth strings and the attention’s output sequences of probability distribution, which is caused by missing or superfluous characters
  - solution & novelty
    - propose a novel method called edit probability (EP) for scene text recognition
    - EP tries to effectively estimate the probability of generating a string from the output sequence of probability distribution conditioned on the input image, while considering the possible occurrences of missing/superfluous characters
    - the advantage lies in that the training process can focus on the missing, superfluous and unrecognized characters, and thus the impact of the misalignment problem can be alleviated or even overcome
  - the devil in the details
    - None 
- [CVPR2018] [AON: Towards Arbitrarily-Oriented Text Recognition](https://arxiv.org/abs/1711.04226)
  - **short summary**
    - good performace on irregular dataset
    - interesting idea
  - annotation
    - word level
  - rectification module
    - no
  - feature map
    - 1d
  - decoder
    - RNN and attention
  - motivation
    - most existing methods directly encode a text image as a 1D sequence of features and then decode them to the predicted text, which implies that any text in an image is treated in the same direction such as from left to right by default. However, this is not true in the wild
  - solution & novelty
    - suggest that the visual representation of an arbitrarily oriented character in a 2D image can be described in four directions: left → right, right → left, top → bottom and bottom → top
  - the devil in the details
    - none
- [ECCV2018] [Synthetically Supervised Feature Learning for
Scene Text Recognition](http://openaccess.thecvf.com/content_ECCV_2018/papers/Yang_Liu_Synthetically_Supervised_Feature_ECCV_2018_paper.pdf)
  - **short summary**
    - add GAN to STR
  - annotation
    - word level
  - rectification module
    - no
  - feature map
    - 1d
  - decoder
    - RNN and attention
  - motivation
    - todo
  - solution & novelty
    - todo
  - the devil in the details
    - none
### 2017
- [ICCV2017] [Focusing Attention: Towards Accurate Text Recognition in Natural Images](https://arxiv.org/abs/1709.02054)
  - **short summary**
    - todo
  - annotation
    - word level
  - rectification module
    - yes
  - feature map
    - 1d
  - decoder
    - RNN and attention
  - motivation
    - existing attention-based methods perform poorly on complicated and/or low-quality images
    - one major reason is that existing methods cannot get accurate alignments between feature areas and targets for such images
  - solution & novelty
    - propose the Focusing Attention Network (FAN) method that employs a focusing attention mechanism to automatically draw back the drifted attention
    - FAN consists of two major components: an attention network (AN) that is responsible for recognizing character targets as in the existing methods, and a focusing network (FN) that is responsible for adjusting attention by evaluating whether AN pays attention properly on the target areas in the images
    - adopt a ResNet-based network to enrich deep representations of scene text images
  - the devil in the details
    - none
### 2016
- [CVPR2016] [Robust Scene Text Recognition with Automatic Rectification
](https://arxiv.org/abs/1603.03915)
  - **short summary**
    - first plugs STN in STR task
    - after CRNN
  - annotation
    - word level
  - rectification module
    - yes
  - feature map
    - 1d
  - decoder
    - RNN and attention
  - motivation
    - words in natural images often possess irregular shapes, which are caused by perspective distortion, curved character placement, etc
  - solution & novelty
    - propose a recification module which is robust to irregular text
  - the devil in the details
    - none
- [PAMI2016] [An End-to-End Trainable Neural Network for Image-based Sequence Recognition and Its Application to Scene Text Recognition](https://arxiv.org/abs/1507.05717)
  - **short summary**
    - CRNN
    - a milestone in deep learning era
  - annotation
    - word level
  - rectification module
    - no
  - feature map
    - 1d
  - decoder
    - CTC
  - motivation
    - most of the existing algorithms' components are separately trained and tuned
    - other algorithms either need characater annotation or need predefined lexicon
  - solution & novelty
    - the first model that combines cnn and rnn, which can be trained end to end
    - only need word level annotation
    - handle sequences in arbitrary lengths
  - the devil in the details
    - none
