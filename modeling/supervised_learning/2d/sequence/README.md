- [Tutorial2019] [HMM](https://web.stanford.edu/~jurafsky/slp3/A.pdf)

- [1906.08237] [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237)

- [1810.04805] [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)

- [NIPS2017] [Attention Is All You Need](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf)

- [Distill2017] [Sequence Modeling
With CTC](https://distill.pub/2017/ctc/)

- [Distill2016] [Attention and Augmented Recurrent Neural Networks](https://distill.pub/2016/augmented-rnns/) # encoder & decoder

- [ICML2016] [Connectionist Temporal Classification: Labelling Unsegmented
Sequence Data with Recurrent Neural Networks](https://www.cs.toronto.edu/~graves/icml_2006.pdf)

- [NIPS2014] [Sequence to Sequence Learning
with Neural Networks
](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)

